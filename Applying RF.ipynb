{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Set the parameters by cross-validation for RF\n",
    "tuned_parameters = {'n_estimators': [200, 300], \n",
    "                    'min_samples_split': [0.01, 0.05, 0.1],\n",
    "                    'min_samples_leaf': [0.01, 0.005, 0.1]}\n",
    "\n",
    "run_time = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   39.1s finished\n",
      " 10%|█         | 1/10 [00:40<06:01, 40.17s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   37.9s finished\n",
      " 20%|██        | 2/10 [01:18<05:17, 39.74s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.1s finished\n",
      " 30%|███       | 3/10 [01:57<04:36, 39.48s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.3s finished\n",
      " 40%|████      | 4/10 [02:36<03:56, 39.36s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.1s finished\n",
      " 50%|█████     | 5/10 [03:15<03:16, 39.24s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.1s finished\n",
      " 60%|██████    | 6/10 [03:55<02:36, 39.24s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.3s finished\n",
      " 70%|███████   | 7/10 [04:34<01:57, 39.21s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.3s finished\n",
      " 80%|████████  | 8/10 [05:13<01:18, 39.17s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.2s finished\n",
      " 90%|█████████ | 9/10 [05:52<00:39, 39.13s/it][Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   38.1s finished\n",
      "100%|██████████| 10/10 [06:31<00:00, 39.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data']\n",
    "Y = iris['target']\n",
    "\n",
    "# Define ratio between testing and training data\n",
    "rtt = 0.1 # test-to-train ratio\n",
    "test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "acc_test_list = np.zeros(run_time)\n",
    "acc_train_list = np.zeros(run_time)\n",
    "\n",
    "for idx_run in tqdm(range(run_time)):\n",
    "    # Split data\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "    train_idx, test_idx = next(sss.split(X, Y))\n",
    "    X_train = X[train_idx]\n",
    "    Y_train = Y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    Y_test = Y[test_idx]\n",
    "\n",
    "    # Cross validate to find best hyper parameters\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                       tuned_parameters, cv=5, verbose=1)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    # Train again with best hyperparameter\n",
    "    clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                 min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                 n_estimators = clf.best_params_['n_estimators'],\n",
    "                                 min_samples_split = clf.best_params_['min_samples_split'])\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    # Prediction\n",
    "    Y_pred_test = clf.predict(X_test)\n",
    "    Y_pred_train = clf.predict(X_train)\n",
    "    acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "    acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "    acc_test_list[idx_run] = acc_test\n",
    "    acc_train_list[idx_run] = acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9571428571428573, 6.541955017301036e-05)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_test_list), np.var(acc_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning Relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/planning_dataset.txt', sep=\"\\t\", header=None)\n",
    "data.columns = [f\"f{x}\" for x in range(14)]\n",
    "data = data.drop(columns=['f13']) # last column is redundant\n",
    "data.f12 = data.f12.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].to_numpy()\n",
    "Y = data.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182, 12), (182,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt = 0.1\n",
    "test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "# Split data\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509)\n",
    "train_idx, test_idx = next(sss.split(X, Y))\n",
    "X_train = X[train_idx]\n",
    "Y_train = Y[train_idx]\n",
    "X_test = X[test_idx]\n",
    "Y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((165, 12), (17, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/seeds_dataset.txt', sep=\"\\t\", header=None)\n",
    "data.columns = [f\"f{x}\" for x in range(8)]\n",
    "data.f7 = data.f7.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1].to_numpy()\n",
    "Y = data.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 7), (210,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/sonar_all.txt', sep=\",\", header=None)\n",
    "data.columns = [f\"f{x}\" for x in range(61)]\n",
    "data.f60 = data.f60.astype('category')\n",
    "data.f60 = data.f60.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 60), (208,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:,:-1].to_numpy()\n",
    "Y = data.iloc[:,-1].to_numpy()\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/wine_data.txt', sep=\",\", header=None)\n",
    "data.columns = [f\"f{x}\" for x in range(14)]\n",
    "data.f0 = data.f0.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:,1:].to_numpy()\n",
    "Y = data.iloc[:,0].to_numpy()\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Set the parameters by cross-validation for RF\n",
    "tuned_parameters = {'n_estimators': [200, 300], \n",
    "                    'min_samples_split': [0.01, 0.05, 0.1],\n",
    "                    'min_samples_leaf': [0.01, 0.005, 0.1]}\n",
    "\n",
    "run_time = 100\n",
    "    \n",
    "def run_iris(run_time=100):\n",
    "    print('----- IRIS DATASET -----')\n",
    "    # Load dataset\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris['data']\n",
    "    Y = iris['target']\n",
    "    \n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                               tuned_parameters, cv=5, verbose=0)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Train again with best hyperparameter\n",
    "            clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                         min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                         n_estimators = clf.best_params_['n_estimators'],\n",
    "                                         min_samples_split = clf.best_params_['min_samples_split'])\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)\n",
    "\n",
    "def run_planning(run_time=100):\n",
    "    # Load dataset\n",
    "    print('----- PLANNING DATASET -----')\n",
    "    data = pd.read_csv('Dataset/planning_dataset.txt', sep=\"\\t\", header=None)\n",
    "    data.columns = [f\"f{x}\" for x in range(14)]\n",
    "    data = data.drop(columns=['f13']) # last column is redundant\n",
    "    data.f12 = data.f12.astype('int64')\n",
    "    X = data.iloc[:,:-1].to_numpy()\n",
    "    Y = data.iloc[:,-1].to_numpy()\n",
    "\n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                               tuned_parameters, cv=5, verbose=0)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Train again with best hyperparameter\n",
    "            clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                         min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                         n_estimators = clf.best_params_['n_estimators'],\n",
    "                                         min_samples_split = clf.best_params_['min_samples_split'])\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)\n",
    "\n",
    "def run_seeds(run_time=100):\n",
    "    # Load dataset\n",
    "    print('----- SEEDS DATASET -----')\n",
    "    data = pd.read_csv('Dataset/seeds_dataset.txt', sep=\"\\t\", header=None)\n",
    "    data.columns = [f\"f{x}\" for x in range(8)]\n",
    "    data.f7 = data.f7.astype('int64')\n",
    "    X = data.iloc[:,:-1].to_numpy()\n",
    "    Y = data.iloc[:,-1].to_numpy()\n",
    "\n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                               tuned_parameters, cv=5, verbose=0)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Train again with best hyperparameter\n",
    "            clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                         min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                         n_estimators = clf.best_params_['n_estimators'],\n",
    "                                         min_samples_split = clf.best_params_['min_samples_split'])\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)\n",
    "\n",
    "def run_sonar(run_time=100):\n",
    "    # Load dataset\n",
    "    print('----- SONAR DATASET -----')\n",
    "    data = pd.read_csv('Dataset/sonar_all.txt', sep=\",\", header=None)\n",
    "    data.columns = [f\"f{x}\" for x in range(61)]\n",
    "    data.f60 = data.f60.astype('category')\n",
    "    data.f60 = data.f60.cat.codes\n",
    "    X = data.iloc[:,:-1].to_numpy()\n",
    "    Y = data.iloc[:,-1].to_numpy()\n",
    "\n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                               tuned_parameters, cv=5, verbose=0)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Train again with best hyperparameter\n",
    "            clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                         min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                         n_estimators = clf.best_params_['n_estimators'],\n",
    "                                         min_samples_split = clf.best_params_['min_samples_split'])\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)\n",
    "    \n",
    "def run_wine(run_time=100):\n",
    "    # Load dataset\n",
    "    print('----- WINE DATASET -----')\n",
    "    data = pd.read_csv('Dataset/wine_data.txt', sep=\",\", header=None)\n",
    "    data.columns = [f\"f{x}\" for x in range(14)]\n",
    "    data.f0 = data.f0.astype('int64')\n",
    "    X = data.iloc[:,1:].to_numpy()\n",
    "    Y = data.iloc[:,0].to_numpy()\n",
    "\n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509), \n",
    "                               tuned_parameters, cv=5, verbose=0)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Train again with best hyperparameter\n",
    "            clf = RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=1509, \n",
    "                                         min_samples_leaf = clf.best_params_['min_samples_leaf'],\n",
    "                                         n_estimators = clf.best_params_['n_estimators'],\n",
    "                                         min_samples_split = clf.best_params_['min_samples_split'])\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_planning(run_time=100):\n",
    "    # Load dataset\n",
    "    print('----- PLANNING DATASET -----')\n",
    "    data = pd.read_csv('Dataset/planning_dataset.txt', sep=\"\\t\", header=None)\n",
    "    data.columns = [f\"f{x}\" for x in range(14)]\n",
    "    data = data.drop(columns=['f13']) # last column is redundant\n",
    "    data.f12 = data.f12.astype('int64')\n",
    "    X = data.iloc[:,:-1].to_numpy()\n",
    "    Y = data.iloc[:,-1].to_numpy()\n",
    "\n",
    "    rtt_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mean_acc_test = np.zeros(len(rtt_list))\n",
    "    mean_var_test = np.zeros(len(rtt_list))\n",
    "    mean_acc_train = np.zeros(len(rtt_list))\n",
    "    mean_var_train = np.zeros(len(rtt_list))\n",
    "    \n",
    "    for idx_rtt, rtt in enumerate(rtt_list):\n",
    "        # Define ratio between testing and training data\n",
    "        # rtt = 0.1 # test-to-train ratio\n",
    "        print(f\"Processing rtt={rtt} ...\")\n",
    "        test_size = rtt * 1 / (1+rtt) \n",
    "\n",
    "        acc_test_list = np.zeros(run_time)\n",
    "        acc_train_list = np.zeros(run_time)\n",
    "\n",
    "        for idx_run in tqdm(range(run_time)):\n",
    "            # Split data\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=1509+idx_run)\n",
    "            train_idx, test_idx = next(sss.split(X, Y))\n",
    "            X_train = X[train_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            Y_test = Y[test_idx]\n",
    "\n",
    "            # Cross validate to find best hyper parameters\n",
    "            clf = RandomForestClassifier(n_estimators=300, n_jobs=-1,random_state=1509, \n",
    "                                         min_samples_split=0.1, min_samples_leaf=0.05)\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            # Prediction\n",
    "            Y_pred_test = clf.predict(X_test)\n",
    "            Y_pred_train = clf.predict(X_train)\n",
    "            acc_test = accuracy_score(Y_test, Y_pred_test)\n",
    "            acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "            acc_test_list[idx_run] = acc_test\n",
    "            acc_train_list[idx_run] = acc_train\n",
    "        m_a_test = np.mean(acc_test_list)\n",
    "        m_a_train = np.mean(acc_train_list)\n",
    "        m_v_test = np.var(acc_test_list)\n",
    "        m_v_train = np.var(acc_train_list)\n",
    "        mean_acc_test[idx_rtt] = round(m_a_test,4)\n",
    "        mean_acc_train[idx_rtt] = round(m_a_train,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_test,4)\n",
    "        mean_var_test[idx_rtt] = round(m_v_train,4)\n",
    "    print(\"Mean Accuracy Test\")\n",
    "    print(mean_acc_test)\n",
    "    print(\"Mean Accuracy Train\")\n",
    "    print(mean_acc_train)\n",
    "    print(\"Var Accuracy Test\")\n",
    "    print(mean_var_test)\n",
    "    print(\"Var Accuracy Train\")\n",
    "    print(mean_var_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- PLANNING DATASET -----\n",
      "Processing rtt=0.1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rtt=0.2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.64it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rtt=0.3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rtt=0.4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.66it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rtt=0.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Test\n",
      "[0.7059 0.7084 0.7124 0.7138 0.7175]\n",
      "Mean Accuracy Train\n",
      "[0.7293 0.744  0.7642 0.7637 0.7649]\n",
      "Var Accuracy Test\n",
      "[0.0001 0.0003 0.0003 0.0005 0.0005]\n",
      "Var Accuracy Train\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_planning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
